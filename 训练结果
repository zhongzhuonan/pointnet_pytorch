损失函数相同：loss_function=nn.CrossEntropyLoss()
B=64
optimizer=torch.optim.Adam(net.parameters(),lr=0.1)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.5) # 对学习率进行调整
每5个epoch的准确率为:
0.07495948136142626
0.14951377633711507
0.15721231766612642
0.1851701782820097
0.21110210696920584
0.21758508914100486
0.2260940032414911
0.23298217179902755
0.2366288492706645
0.24230145867098865
*****************开始测试*****************
final accuracy 0.23743922204213938

B=64
optimizer=torch.optim.Adam(net.parameters(),lr=0.1)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.1) # 对学习率进行调整
每5个epoch的准确率为:
0.05915721231766612
0.13330632090761751
0.1385737439222042
0.14424635332252836
0.14019448946515398
0.14708265802269044
0.13897893030794164
0.1454619124797407
0.14019448946515398
0.14222042139384117
*****************开始测试*****************
final accuracy 0.14181523500810372

B=32
optimizer=torch.optim.Adam(net.parameters(),lr=0.1)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.1) # 对学习率进行调整
每5个epoch的准确率为:
0.06807131280388978
0.10251215559157212
0.10048622366288493
0.10818476499189628
0.10899513776337115
0.10413290113452188
0.10656401944894651
0.10494327390599675
0.10656401944894651
0.10291734197730956
*****************开始测试*****************
final accuracy 0.10413290113452188


难道是学习率下降没有生效？
B=64
optimizer=torch.optim.Adam(net.parameters(),lr=1)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.1) # 对学习率进行调整
每5个epoch的准确率为:
0.04051863857374392
0.04051863857374392
0.04051863857374392
0.04051863857374392
0.04051863857374392
0.04051863857374392
0.04051863857374392
0.04051863857374392
0.04051863857374392
0.04051863857374392
*****************开始测试*****************
final accuracy 0.04051863857374392

B=64
optimizer=torch.optim.Adam(net.parameters(),lr=0.01)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.5) # 对学习率进行调整
每5个epoch的准确率为:
0.06645056726094004
0.14303079416531606
0.21555915721231766
0.2552674230145867
0.26742301458670986
0.26823338735818475
0.27431118314424635
0.28282009724473256
0.2965964343598055
0.28889789303079416
*****************开始测试*****************
final accuracy 0.29294975688816854

B=64
optimizer=torch.optim.Adam(net.parameters(),lr=0.01,weight_decay=1e-4)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.5) # 对学习率进行调整
loss_function=nn.CrossEntropyLoss()
每5个epoch的准确率为:
0.05551053484602917
0.1973257698541329
0.20502431118314424
0.276742301458671
0.31280388978930307
0.3565640194489465
0.3658833063209076
0.3666936790923825
0.3796596434359806
0.3905996758508914
*****************开始测试*****************
final accuracy 0.3889789303079417

B=64
optimizer=torch.optim.Adam(net.parameters(),lr=0.01,betas=(0.9, 0.999),weight_decay=1e-4)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.5) # 对学习率进行调整
loss_function=nn.CrossEntropyLoss()
每5个epoch的准确率为:
0.06361426256077796
0.15235008103727715
0.1859805510534846
0.2171799027552674
0.21474878444084278
0.22366288492706646
0.23419773095623986
0.23905996758508913
0.2532414910858995
0.24675850891410048
*****************开始测试*****************
final accuracy 0.24351701782820098


没有将随机种子固定。
B=64
optimizer=torch.optim.Adam(net.parameters(),lr=0.01,weight_decay=1e-4)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.5) # 对学习率进行调整
loss_function=nn.CrossEntropyLoss()
每5个epoch的准确率为:
0.06766612641815235
0.1458670988654781
0.19611021069692058
0.21110210696920584
0.2293354943273906
0.2386547811993517
0.25364667747163694
0.2576985413290113
0.28038897893030795
0.2901134521880065
*****************开始测试*****************
final accuracy 0.2884927066450567


###        return F.log_softmax(x,dim=1)网络输出做了改动。
但是cross_entropy不是softmax+Log+nLLLoss??

optimizer=torch.optim.Adam(net.parameters(),lr=0.01,weight_decay=1e-4)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.5) # 对学习率进行调整
loss_function=nn.NLLLoss()
每5个epoch的准确率为:
0.06847649918962723
0.14505672609400325
0.15842787682333873
0.2860615883306321
0.37317666126418153
0.37763371150729336
0.4226094003241491
0.4461102106969206
0.4651539708265802
0.46312803889789306
*****************开始测试*****************
final accuracy 0.4692058346839546

B=64 同上其他
optimizer=torch.optim.Adam(net.parameters(),lr=0.001,weight_decay=1e-4)
scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.5) # 对学习率进行调整
loss_function=nn.NLLLoss()
每5个epoch的准确率为:
0.06888168557536467
0.10575364667747164
0.11426256077795786
0.15235008103727715
0.16166936790923825
0.16288492706645058
0.17098865478119935
0.1839546191247974
0.17382495948136142
0.18841166936790923
*****************开始测试*****************
final accuracy 0.18435980551053485


    optimizer=torch.optim.Adam(net.parameters(),lr=0.01,weight_decay=2e-4)
    scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.9) # 对学习率进行调整
epoch=100
每5个epoch的准确率为:
0.05915721231766612
0.1507293354943274
0.19165316045380876
0.24108589951377635
0.24756888168557536
0.3675040518638574
0.4205834683954619
0.4619124797406807
0.42949756888168555
0.5388978930307942
0.570097244732577
0.5964343598055105
0.6243922204213939
0.6207455429497569
0.6385737439222042
0.6446515397082658
0.6555915721231766
0.6438411669367909
0.656807131280389
0.6685575364667747
*****************开始测试*****************
final accuracy 0.6722042139384117

已经能复现结果
每5个epoch的准确率为:
0.06361426256077796
0.14059967585089142
0.19813614262560778
0.22082658022690438
0.21110210696920584
0.24311183144246354
0.22852512155591573
0.2544570502431118
0.3419773095623987
0.36142625607779577
0.4019448946515397
0.4558346839546191
0.44408427876823336
0.47568881685575365
0.46110210696920584
0.4675850891410049
0.4821717990275527
0.49513776337115073
0.4854132901134522
0.4931118314424635
*****************开始测试*****************
final accuracy 0.49270664505672607


    optimizer=torch.optim.Adam(net.parameters(),lr=0.01,weight_decay=2e-4)
    scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=3,gamma=0.9) # 对学习率进行调整
epoch=121
每5个epoch的准确率为:
0.06361426256077796
0.14303079416531606
0.14505672609400325
0.163290113452188
0.3079416531604538
0.35696920583468394
0.35291734197730956
0.3594003241491086
0.3893841166936791
0.37925445705024313
0.43719611021069693
0.4801458670988655
0.48500810372771475
0.5206645056726094
0.5287682333873582
0.5259319286871961
0.5522690437601296
0.5777957860615883
0.5919773095623987
0.583063209076175
0.609805510534846
0.6122366288492707
0.5883306320907618
0.6057536466774717
0.6215559157212318
*****************开始测试*****************
final accuracy 0.6085899513776337


1080训练
    batch_size=64
    root=r'/home/sirb/Documents/ModelNet40_ply'
    train_data=ModelNet40(root=root,sample_points=2048,split='train')
    optimizer=torch.optim.Adam(net.parameters(),lr=0.01,betas=(0.9,0.999))
    scheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.9) # 对学习率进行调整
    loss_function=nn.NLLLoss()
    epochs=101
每5个epoch的准确率为:
0.0713128038897893
0.2273095623987034
0.3719611021069692
0.46434359805510533
0.557131280388979
0.563614262560778
0.5721231766612642
0.5944084278768234
0.6519448946515397
0.6292544570502431
0.6316855753646677
0.6839546191247974
0.6649108589951378
0.6847649918962723
0.6762560777957861
0.6782820097244733
0.6969205834683955
0.6948946515397083
0.7062398703403565
0.7115072933549432
0.7082658022690438
*****************开始测试*****************
final accuracy 0.6908427876823339


